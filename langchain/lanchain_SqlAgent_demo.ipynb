{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1290f794-9477-4e45-ab20-81d04c040fc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## langcain agent demo for ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf80354-7edd-4403-9c8b-97a4712f4dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain[all]\n",
    "!pip install langchain-experimental\n",
    "#!pip install sagemaker --upgrade\n",
    "!pip install  boto3\n",
    "!pip install requests_aws4auth\n",
    "!pip install opensearch-py\n",
    "!pip install pydantic==1.10.0\n",
    "#!pip install PyAthena[SQLAlchemy]==1.0.0\n",
    "#!pip install PyAthena[JDBC]==1.0.0\n",
    "#!pip install openai\n",
    "!pip install sqlalchemy-redshift\n",
    "!pip install redshift_connector\n",
    "!pip install SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a8478-64c7-4ccd-8bb7-4a9b0c71ab7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install SQLAlchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b216f11-a2b5-4f0b-b4de-060a1a64076a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## initial sagemaker env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f05414-d44b-464e-a0c7-46c317378ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::687912291502:role/service-role/AmazonSageMaker-ExecutionRole-20211013T113123\n",
      "sagemaker bucket: sagemaker-us-west-2-687912291502\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "from typing import Dict\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3aed78-e14b-4d30-93d3-2871d6603bd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## intial lanchain lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4aae9f94-a8df-49b5-b22b-f89e8d756b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import ConversationBufferWindowMemory,ConversationBufferMemory\n",
    "from langchain import LLMChain\n",
    "from typing import Any, Dict, List, Union,Mapping, Optional, TypeVar, Union\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"]= \"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\"\n",
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "aos_endpoint=\"vpc-llm-rag-aos-seg3mzhpp76ncpxezdqtcsoiga.us-west-2.es.amazonaws.com\"\n",
    "region='us-west-2'\n",
    "username=\"admin\"\n",
    "passwd=\"(OL>0p;/\"\n",
    "index_name=\"metadata-index\"\n",
    "size=10\n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "region = boto3.Session().region_name\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, 'es', session_token=credentials.token)\n",
    "pwdauth = (username, passwd)\n",
    "\n",
    "### for sqlcoder\n",
    "class TextGenContentHandler2(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": model_kwargs\n",
    "            })\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        #print(response_json)\n",
    "        #sql_result=response_json[\"outputs\"].split(\"```sql\")[-1].split(\"```\")[0].split(\";\")[0].strip().replace(\"\\\\n\",\" \") + \";\"\n",
    "        sql_result=response_json[\"outputs\"]\n",
    "        return sql_result\n",
    "\n",
    "\n",
    "content_hander2=TextGenContentHandler2()\n",
    "\n",
    "## for embedding\n",
    "class EmbeddingContentHandler(EmbeddingsContentHandler):\n",
    "    parameters = {\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"temperature\": 0,\n",
    "        \"min_length\": 10,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "    }\n",
    "    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": inputs, **model_kwargs})\n",
    "        return input_str.encode('utf-8')\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"sentence_embeddings\"]\n",
    "\n",
    "embedding_content_handler=EmbeddingContentHandler()\n",
    "    \n",
    "sm_embeddings = SagemakerEndpointEmbeddings(\n",
    "    # endpoint_name=\"endpoint-name\", \n",
    "    # credentials_profile_name=\"credentials-profile-name\", \n",
    "    #endpoint_name=\"huggingface-textembedding-bloom-7b1-fp1-2023-04-17-03-31-12-148\", \n",
    "    endpoint_name=\"bge-zh-15-2023-09-25-07-02-01-080-endpoint\",\n",
    "    region_name=\"us-west-2\", \n",
    "    content_handler=embedding_content_handler\n",
    ")\n",
    "\n",
    "\n",
    "parameters = {\n",
    "  \"max_new_tokens\": 450,\n",
    "  #\"do_sample\":False,\n",
    "  #\"temperatual\" : 0\n",
    "  #\"no_repeat_ngram_size\": 2,\n",
    "}\n",
    "sm_sql_llm=SagemakerEndpoint(\n",
    "        endpoint_name=\"sqlcoder-2023-09-24-06-31-09-959-endpoint\",\n",
    "        region_name=\"us-west-2\", \n",
    "        model_kwargs=parameters,\n",
    "        content_handler=content_hander2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9330b-0204-4e5a-bc7b-36ec7fd20818",
   "metadata": {
    "tags": []
   },
   "source": [
    "## func for agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b0819cc2-5be1-4f78-8826-90c3ff2c72d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import os\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import LLMChain\n",
    "\n",
    "\n",
    "\n",
    "def aos_knn_search(client, field,q_embedding, index, size=1):\n",
    "    if not isinstance(client, OpenSearch):   \n",
    "        client = OpenSearch(\n",
    "            hosts=[{'host': aos_endpoint, 'port': 443}],\n",
    "            http_auth = pwdauth,\n",
    "            use_ssl=True,\n",
    "            verify_certs=True,\n",
    "            connection_class=RequestsHttpConnection\n",
    "        )\n",
    "    query = {\n",
    "        \"size\": size,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                field: {\n",
    "                    \"vector\": q_embedding,\n",
    "                    \"k\": size\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    opensearch_knn_respose = []\n",
    "    query_response = client.search(\n",
    "        body=query,\n",
    "        index=index\n",
    "    )\n",
    "    opensearch_knn_respose = [{'idx':item['_source'].get('idx',1),'database_name':item['_source']['database_name'],'table_name':item['_source']['table_name'],'query_desc_text':item['_source']['query_desc_text'],\"score\":item[\"_score\"]}  for item in query_response[\"hits\"][\"hits\"]]\n",
    "    return opensearch_knn_respose\n",
    "\n",
    "def aos_reverse_search(client, index_name, field, query_term, exactly_match=False, size=1):\n",
    "    \"\"\"\n",
    "    search opensearch with query.\n",
    "    :param host: AOS endpoint\n",
    "    :param index_name: Target Index Name\n",
    "    :param field: search field\n",
    "    :param query_term: query term\n",
    "    :return: aos response json\n",
    "    \"\"\"\n",
    "    if not isinstance(client, OpenSearch):   \n",
    "        client = OpenSearch(\n",
    "            hosts=[{'host': aos_endpoint, 'port': 443}],\n",
    "            http_auth = pwdauth,\n",
    "            use_ssl=True,\n",
    "            verify_certs=True,\n",
    "            connection_class=RequestsHttpConnection\n",
    "        )\n",
    "    query = None\n",
    "    if exactly_match:\n",
    "        query =  {\n",
    "            \"query\" : {\n",
    "                \"match_phrase\":{\n",
    "                    \"doc\": {\n",
    "                        \"query\": query_term,\n",
    "                        \"analyzer\": \"ik_smart\"\n",
    "                      }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        query = {\n",
    "            \"size\": size,\n",
    "            \"query\": {\n",
    "                \"query_string\": {\n",
    "                \"default_field\": \"query_desc_text\",  \n",
    "                \"query\": query_term         \n",
    "              }\n",
    "            },\n",
    "           \"sort\": [{\n",
    "               \"_score\": {\n",
    "                   \"order\": \"desc\"\n",
    "               }\n",
    "           }]\n",
    "    }        \n",
    "    query_response = client.search(\n",
    "        body=query,\n",
    "        index=index_name\n",
    "    )\n",
    "    result_arr = [{'idx':item['_source'].get('idx',1),'database_name':item['_source']['database_name'],'table_name':item['_source']['table_name'],'query_desc_text':item['_source']['query_desc_text'],\"score\":item[\"_score\"]}  for item in query_response[\"hits\"][\"hits\"]]\n",
    "    return result_arr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_vector_by_sm_endpoint(questions, sm_client, endpoint_name):\n",
    "    parameters = {\n",
    "    }\n",
    "\n",
    "    response_model = sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(\n",
    "            {\n",
    "                \"inputs\": questions,\n",
    "                \"parameters\": parameters,\n",
    "                \"is_query\" : True,\n",
    "                \"instruction\" :  \"为这个句子生成表示以用于检索相关文章：\"\n",
    "            }\n",
    "        ),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    json_str = response_model['Body'].read().decode('utf8')\n",
    "    json_obj = json.loads(json_str)\n",
    "    embeddings = json_obj['sentence_embeddings']\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_topk_items(opensearch_query_response, topk=5):\n",
    "    opensearch_knn_nodup = []\n",
    "    unique_ids = set()\n",
    "    for item in opensearch_query_response:\n",
    "        if item['id'] not in unique_ids:\n",
    "            opensearch_knn_nodup.append(item['score'], item['idx'],item['database_name'],item['table_name'],item['query_desc_text'])\n",
    "            unique_ids.add(item['id'])\n",
    "    return opensearch_knn_nodup\n",
    "\n",
    "\n",
    "\n",
    "def k_nn_ingestion_by_aos(docs,index,hostname,username,passwd):\n",
    "    auth = (username, passwd)\n",
    "    search = OpenSearch(\n",
    "        hosts = [{'host': aos_endpoint, 'port': 443}],\n",
    "        ##http_auth = awsauth ,\n",
    "        http_auth = auth ,\n",
    "        use_ssl = True,\n",
    "        verify_certs = True,\n",
    "        connection_class = RequestsHttpConnection\n",
    "    )\n",
    "    for doc in docs:\n",
    "        query_desc_embedding = doc['query_desc_embedding']\n",
    "        database_name = doc['database_name']\n",
    "        table_name = doc['table_name']\n",
    "        query_desc_text = doc[\"query_desc_text\"]\n",
    "        document = { \"query_desc_embedding\": query_desc_embedding, 'database_name':database_name, \"table_name\": table_name,\"query_desc_text\":query_desc_text}\n",
    "        search.index(index=index, body=document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5728d-e84a-487f-a643-4297954f7631",
   "metadata": {},
   "source": [
    "* for local test only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5da7a2d7-f9db-4420-ac31-c9873f59fcc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'idx': 1, 'database_name': 'llm', 'table_name': 'ads_bi_quality_monitor_shipping_detail', 'query_desc_text': '最近一个月温度合格的派车单数量', 'score': 0.94651675}]\n",
      "[{'idx': 1, 'database_name': 'llm', 'table_name': 'ads_bi_quality_monitor_shipping_detail', 'query_desc_text': '最近一个月温度合格的派车单数量', 'score': 1.4384104}]\n"
     ]
    }
   ],
   "source": [
    "query=\"上个月温度合格的派车单数量\"\n",
    "embedding_endpoint_name=\"bge-zh-15-2023-09-25-07-02-01-080-endpoint\"\n",
    "query_embedding = get_vector_by_sm_endpoint(query, sm_client, embedding_endpoint_name)\n",
    "\n",
    "client=None\n",
    "rets=aos_knn_search(client, \"query_desc_embedding\",query_embedding[0], index_name, size=1)\n",
    "print(rets)\n",
    "\n",
    "rets=opensearch_query_response = aos_reverse_search(client, index_name, \"query_desc_text\", query)   \n",
    "print(rets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa302e0a-b8ed-4fa0-934e-ab4ff0740316",
   "metadata": {
    "tags": []
   },
   "source": [
    "## major chain pipeline ################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ca53df-0f55-46ec-85a9-36316e25008b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 0: index 创建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ccff8-4361-4f77-a42f-9d2c64ebe387",
   "metadata": {},
   "source": [
    "PUT metadata-index\n",
    "{\n",
    "    \"settings\" : {\n",
    "        \"index\":{\n",
    "            \"number_of_shards\" : 5,\n",
    "            \"number_of_replicas\" : 0,\n",
    "            \"knn\": \"true\",\n",
    "            \"knn.algo_param.ef_search\": 32\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"metadata_type\" : {\n",
    "                \"type\" : \"keyword\"\n",
    "            },\n",
    "            \"database_name\": {\n",
    "                \"type\" : \"keyword\"\n",
    "            },\n",
    "            \"table_name\": {\n",
    "               \"type\" : \"keyword\"\n",
    "            },\n",
    "            \"query_desc_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"ik_max_word\",\n",
    "                \"search_analyzer\": \"ik_smart\"\n",
    "            },\n",
    "            \"query_desc_embedding\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 768,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"l2\",\n",
    "                    \"engine\": \"faiss\",\n",
    "                    \"parameters\": {\n",
    "                        \"ef_construction\": 512,\n",
    "                        \"m\": 32\n",
    "                    }\n",
    "                }            \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ef73a-d691-4d30-b35e-1a5254cdb8d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1: data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9bb9c564-9d44-4ba3-a279-34b6df2aa3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_querys = \"\"\"最近一个月温度合格的派车单数量\"\"\"\n",
    "querys = all_querys.split(\"\\n\")\n",
    "\n",
    "all_tables = \"\"\"ads_bi_quality_monitor_shipping_detail\"\"\"\n",
    "tables=all_tables.split(\"\\n\")\n",
    "\n",
    "all_dbs = \"\"\"llm\"\"\"\n",
    "dbs=all_dbs.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a8099c8-53e1-4581-9e87-381f5cf90020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"./code/\")\n",
    "#import func\n",
    "\n",
    "embedding_endpoint_name=\"bge-zh-15-2023-09-25-07-02-01-080-endpoint\"\n",
    "##########embedding by llm model##############\n",
    "sentense_vectors = []\n",
    "sentense_vectors=get_vector_by_sm_endpoint(querys,sm_client,embedding_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "793a20d5-fdd0-4be2-835d-8afc753062a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs=[]\n",
    "for index, sentence_vector in enumerate(sentense_vectors):\n",
    "    #print(index, sentence_vector)\n",
    "    doc = {\n",
    "        \"metadata_type\":\"table\",\n",
    "        \"database_name\":dbs[index],\n",
    "        \"table_name\": tables[index],\n",
    "        \"query_desc_text\":querys[index],\n",
    "        \"query_desc_embedding\": sentence_vector\n",
    "          }\n",
    "    docs.append(doc)\n",
    "\n",
    "#print((doc[\"database_name\"]))\n",
    "#########ingestion into aos ###################\n",
    "k_nn_ingestion_by_aos(docs,index_name,aos_endpoint,username,passwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b98b6e5-4ee6-4749-8aa5-c7c8a48b95ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2:自定义Agent ，定制tools\n",
    "* 自定义AOS倒排及knn检索tools    \n",
    "* 自定义中文Sql Agent 的ReAct prompt 前缀   \n",
    "* 定制CustomerizedSqlDatabaseChain作为db tools做数据库交互\n",
    "* 将db tools加入之前定义的元数据召回的tools列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "beac4e51-32b7-460f-adee-b20151163e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.tools.base import BaseTool, Tool, tool\n",
    "from langchain.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from typing import Optional, Type\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    "    CallbackManagerForChainRun\n",
    ")\n",
    "\n",
    "\n",
    "name=\"embedding knn search\"\n",
    "description=\"用于标签检索找到具体的数据库和表名\"\n",
    "aos_client = OpenSearch(\n",
    "            hosts=[{'host': aos_endpoint, 'port': 443}],\n",
    "            http_auth = pwdauth,\n",
    "            use_ssl=True,\n",
    "            verify_certs=True,\n",
    "            connection_class=RequestsHttpConnection)\n",
    "aos_index=\"metadata-index\"\n",
    "\n",
    "\"\"\"Opensearch 向量检索.\"\"\"\n",
    "def customEmbeddingSearch(query: str) -> str:\n",
    "    start = time.time()\n",
    "    query_embedding = get_vector_by_sm_endpoint(query, sm_client, embedding_endpoint_name)\n",
    "    elpase_time = time.time() - start\n",
    "    #print(f'runing time of opensearch_knn : {elpase_time}s seconds')\n",
    "    responses = aos_knn_search(aos_client, \"query_desc_embedding\",query_embedding[0], aos_index, size=10)\n",
    "    #return \"{table name: \"+responses[0][\"table_name\"] +\"}\" \n",
    "    return responses[0][\"table_name\"] \n",
    "\n",
    "\"\"\"Opensearch 标签检索.\"\"\"\n",
    "def customReverseIndexSearch(query: str) -> str:\n",
    "    start = time.time()\n",
    "    opensearch_query_response = aos_reverse_search(aos_client, aos_index, \"query_desc_text\", query)\n",
    "    elpase_time = time.time() - start\n",
    "    #print(f'runing time of opensearch_query : {elpase_time}s seconds')\n",
    "    #return \"{table name: \"+opensearch_query_response[0][\"table_name\"] +\"}\" \n",
    "    return opensearch_query_response[0][\"table_name\"] \n",
    "\n",
    " \n",
    "db = SQLDatabase.from_uri(\n",
    "    \"mysql+pymysql://admin:admin12345678@database-us-west-2-demo.cluster-c1qvx9wzmmcz.us-west-2.rds.amazonaws.com/llm\",\n",
    "    sample_rows_in_table_info=0)    \n",
    "    \n",
    "class CustomerizedSQLDatabaseChain(SQLDatabaseChain):\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
    "        input_text = f\"{inputs[self.input_key]}\\nSQLQuery:\"\n",
    "        _run_manager.on_text(input_text, verbose=self.verbose)\n",
    "        # If not present, then defaults to None which is all tables.\n",
    "        table_names_to_use = inputs.get(\"table_names_to_use\")\n",
    "        print(\"table name==\")\n",
    "        print(self.database._include_tables)\n",
    "        table_info = self.database.get_table_info(table_names=table_names_to_use)\n",
    "        llm_inputs = {\n",
    "            \"input\": input_text,\n",
    "            \"top_k\": str(self.top_k),\n",
    "            \"dialect\": self.database.dialect,\n",
    "            \"table_info\": table_info,\n",
    "            #\"stop\": [\"\\nSQLResult:\"],\n",
    "        }\n",
    "        intermediate_steps: List = []\n",
    "        try:\n",
    "            intermediate_steps.append(llm_inputs)  # input: sql generation\n",
    "            sql_cmd = self.llm_chain.predict(\n",
    "                callbacks=_run_manager.get_child(),\n",
    "                **llm_inputs,\n",
    "            ).strip()\n",
    "            print(\"orginal sql_cmd==\"+sql_cmd)\n",
    "            if self.return_sql:\n",
    "                return {self.output_key: sql_cmd}\n",
    "            if not self.use_query_checker:\n",
    "                _run_manager.on_text(sql_cmd, color=\"green\", verbose=self.verbose)\n",
    "                intermediate_steps.append(\n",
    "                    sql_cmd\n",
    "                )  # output: sql generation (no checker)\n",
    "                #########定制sqlcoder 模型输出##############\n",
    "                pattern = r\"SQLQuery: (.*?)\\n\"\n",
    "                matches = re.findall(pattern, sql_cmd)\n",
    "                match = matches[1]\n",
    "                sql_cmd = match\n",
    "                #print(\"query sql==\"+sql_cmd) \n",
    "                \n",
    "                intermediate_steps.append({\"sql_cmd\": sql_cmd})  # input: sql exec\n",
    "                result = self.database.run(sql_cmd)\n",
    "                intermediate_steps.append(str(result))  # output: sql exec\n",
    "            else:\n",
    "                query_checker_prompt = self.query_checker_prompt or PromptTemplate(\n",
    "                    template=QUERY_CHECKER, input_variables=[\"query\", \"dialect\"]\n",
    "                )\n",
    "                query_checker_chain = LLMChain(\n",
    "                    llm=self.llm_chain.llm, prompt=query_checker_prompt\n",
    "                )\n",
    "                query_checker_inputs = {\n",
    "                    \"query\": sql_cmd,\n",
    "                    \"dialect\": self.database.dialect,\n",
    "                }\n",
    "                checked_sql_command: str = query_checker_chain.predict(\n",
    "                    callbacks=_run_manager.get_child(), **query_checker_inputs\n",
    "                ).strip()\n",
    "                intermediate_steps.append(\n",
    "                    checked_sql_command\n",
    "                )  # output: sql generation (checker)\n",
    "                _run_manager.on_text(\n",
    "                    checked_sql_command, color=\"green\", verbose=self.verbose\n",
    "                )\n",
    "                intermediate_steps.append(\n",
    "                    {\"sql_cmd\": checked_sql_command}\n",
    "                )  # input: sql exec\n",
    "                result = self.database.run(checked_sql_command)\n",
    "                intermediate_steps.append(str(result))  # output: sql exec\n",
    "                sql_cmd = checked_sql_command\n",
    "\n",
    "            _run_manager.on_text(\"\\nSQLResult: \", verbose=self.verbose)\n",
    "            _run_manager.on_text(result, color=\"yellow\", verbose=self.verbose)\n",
    "            # If return direct, we just set the final result equal to\n",
    "            # the result of the sql query result, otherwise try to get a human readable\n",
    "            # final answer\n",
    "            if self.return_direct:\n",
    "                final_result = result\n",
    "            else:\n",
    "                _run_manager.on_text(\"\\nAnswer:\", verbose=self.verbose)\n",
    "                input_text += f\"{sql_cmd}\\nSQLResult: {result}\\nAnswer:\"\n",
    "                llm_inputs[\"input\"] = input_text\n",
    "                intermediate_steps.append(llm_inputs)  # input: final answer\n",
    "                final_result = self.llm_chain.predict(\n",
    "                    callbacks=_run_manager.get_child(),\n",
    "                    **llm_inputs,\n",
    "                ).strip()\n",
    "                intermediate_steps.append(final_result)  # output: final answer\n",
    "                _run_manager.on_text(final_result, color=\"green\", verbose=self.verbose)\n",
    "            chain_result: Dict[str, Any] = {self.output_key: final_result}\n",
    "            if self.return_intermediate_steps:\n",
    "                chain_result[INTERMEDIATE_STEPS_KEY] = intermediate_steps\n",
    "            return chain_result\n",
    "        except Exception as exc:\n",
    "            # Append intermediate steps to exception, to aid in logging and later\n",
    "            # improvement of few shot prompt seeds\n",
    "            exc.intermediate_steps = intermediate_steps  # type: ignore\n",
    "            raise exc\n",
    "\n",
    "def run_query(query):\n",
    "    print(\"db tool input==\"+query)\n",
    "    #table_name=json.loads(query)[\"table_name\"]\n",
    "    table_name=\"ads_bi_quality_monitor_shipping_detail\"\n",
    "    #PROMPT_sql = PromptTemplate(\n",
    "    #    input_variables=[\"question\", \"table_info\"], template=sql_prompt_template\n",
    "    #)\n",
    "    db_chain = CustomerizedSQLDatabaseChain.from_llm(sm_sql_llm, db, verbose=True, return_intermediate_steps=False)\n",
    "    db_chain.database._include_tables=[table_name]\n",
    "    response=db_chain.run(query)\n",
    "    return response\n",
    "\n",
    "\n",
    "custom_tool_list=[\n",
    "    Tool.from_function(\n",
    "        func=customReverseIndexSearch,\n",
    "        name=\"reverse index search\",\n",
    "        description=\"用于向量检索找到具体的数据库和表名\"\n",
    "    ),\n",
    "    Tool.from_function(\n",
    "        func=customEmbeddingSearch,\n",
    "        name=\"embedding knn search\",\n",
    "        description=\"用于标签检索找到具体的数据库和表名\"\n",
    "    ),\n",
    "    Tool.from_function(\n",
    "        name=\"Db Querying\",\n",
    "        func=run_query,\n",
    "        description=\"\"\"用于数据库交互及生成sql\"\"\"\n",
    "    )]\n",
    "\n",
    "#reverseIndexSearchTool=CustomReverseIndexSearchTool()\n",
    "#embeddingSearchTool=CustomEmbeddingSearchTool()\n",
    "\n",
    "#custom_tool_list=[]\n",
    "#custom_tool_list=[CustomReverseIndexSearchTool(),CustomEmbeddingSearchTool()]\n",
    "#custom_tool_list.append(\n",
    "#   Tool.from_function(\n",
    "#        name=\"Db Querying Tool\",\n",
    "#        func=run_query,\n",
    "#        description=\"\"\"用于数据库交互及生成sql\"\"\"\n",
    "#    ))\n",
    "\n",
    "\n",
    "#print(custom_tool_list)\n",
    "\n",
    "\n",
    "### for llama2\n",
    "class TextGenContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": model_kwargs\n",
    "            })\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"generated_text\"]\n",
    "\n",
    "\n",
    "content_hander=TextGenContentHandler()\n",
    "\n",
    "sm_llm=SagemakerEndpoint(\n",
    "        endpoint_name=\"tgi-llama2-2023-09-27-14-25-28-609-endpoint\",\n",
    "        region_name=\"us-west-2\", \n",
    "        model_kwargs=parameters,\n",
    "        content_handler=content_hander\n",
    ")\n",
    "\n",
    "\n",
    "## for bedrock\n",
    "def get_bedrock_aksk(secret_name='chatbot_bedrock', region_name = \"us-west-2\"):\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        # For a list of exceptions thrown, see\n",
    "        # https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "        raise e\n",
    "\n",
    "    # Decrypts secret using the associated KMS key.\n",
    "    secret = json.loads(get_secret_value_response['SecretString'])\n",
    "    return secret['BEDROCK_ACCESS_KEY'],secret['BEDROCK_SECRET_KEY']\n",
    "\n",
    "ACCESS_KEY, SECRET_KEY=get_bedrock_aksk()\n",
    "\n",
    "boto3_bedrock = boto3.client(\n",
    "    service_name=\"bedrock\",\n",
    "    region_name=\"us-east-1\",\n",
    "    endpoint_url=\"https://bedrock.us-west-2.amazonaws.com\",\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY\n",
    ")\n",
    "\n",
    "parameters_bedrock = {\n",
    "    \"max_tokens_to_sample\": 450,\n",
    "    #\"stop_sequences\":STOP,\n",
    "    #\"temperature\":0.5,\n",
    "    # \"top_p\":0.9\n",
    "}\n",
    "\n",
    "sm_llm = Bedrock(model_id=\"anthropic.claude-v2\", client=boto3_bedrock, model_kwargs=parameters)\n",
    "\n",
    "\n",
    "\n",
    "####目前SqlAgent只支持OpenAI Function类型，不适合SM endpoint######\n",
    "#toolkit = SQLDatabaseToolkit(db=db, llm=sm_sql_llm)\n",
    "\n",
    "#custom_suffix = \"\"\"\n",
    "#我应该先利用标签检索，找到具体的数据库和表名，\n",
    "#如果找不到，则利用向量检索查找，\n",
    "#然后使用数据库工具查看我刚才找到的应该查询的库和表的详细schema元数据。\n",
    "#\"\"\"\n",
    "#agent = create_sql_agent(llm=sm_llm,\n",
    "#                         toolkit=toolkit,\n",
    "#                         verbose=True,\n",
    "#                         agent_type=AgentType.SELF_ASK_WITH_SEARCH,\n",
    "#                         extra_tools=custom_tool_list,\n",
    "#                         suffix=custom_suffix\n",
    "#                        )\n",
    "#print(agent.agent.llm_chain.prompt.template)\n",
    "#agent.run({\"input\":\"我需要知道销售报表中，下单金额最大的客户id\",\"agent_scratchpad\":\"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf8dfb-6487-4679-bd40-ec2d1d6693d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## llm_chain自定义 Agent test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6fb3ec-2eb0-483a-8c88-1bbd06781488",
   "metadata": {
    "tags": []
   },
   "source": [
    "* 使用llm chain做chat的planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "81c9e6f8-95fe-42ec-8ddb-b6e0658676ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pydantic import Extra\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.prompts.base import BasePromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner\n",
    "from langchain.agents.tools import Tool\n",
    "\n",
    "\n",
    "sql_prompt_template = \"\"\"\n",
    "You are a MySQL expert. Given an input question, first create a syntactically correct MySql query to run, then look at the results of the query and return the answer to the input question.\n",
    "Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per MySQL. \n",
    "You can order the results to return the most informative data in the database.\n",
    "Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (`) to denote them as delimited identifiers.\n",
    "Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "Pay attention to use CURDATE() function to get the current date, if the question involves \"today\".\n",
    "\n",
    "Use the following format:\n",
    "Question: Question here\n",
    "SQLQuery: SQL Query to run\n",
    "SQLResult: Result of the SQLQuery\n",
    "Answer: Final answer here\n",
    "\n",
    "Only use the following tables:\n",
    "{table_info}\n",
    "\n",
    "Question: {question}\n",
    "SQLQuery:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "def build_chat_chain():\n",
    "    conversation_with_summary_chain = ConversationChain(\n",
    "        llm=sm_llm, \n",
    "        verbose=True\n",
    "    )\n",
    "    return conversation_with_summary_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea0f676-5fee-40a7-a1e1-71aef0af09cc",
   "metadata": {},
   "source": [
    "* 使用llm chain做chat的planner\n",
    "* 使用langchain plannerAndExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b00cf-8fcd-445f-82db-4b0d019096a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "combo_template = \"\"\"\n",
    "先了解问题并制定解决问题的计划。\n",
    "请输出以标题\"plan:\"开头的计划，然后输出步骤的编号列表。在计划步骤时，不要使用过去的对话历史记录。\n",
    "请将计划制定为准确完成任务所需的最少步骤数。\n",
    "\n",
    "以下是使用工具解决任务的步骤，请严格遵守：\n",
    "先使用\"reverse index search\" 标签检索工具查找输入问题。\n",
    "如果结果为空，则使用\"embedding knn search\" 向量检索工具查找输入问题。\n",
    "然后使用\"db Querying Tool\" 数据库工具执行上一步骤结果输出。\n",
    "最后返回结果。\n",
    "\"\"\"\n",
    "\n",
    "combo_template = \"\"\"\n",
    "First understand the problem and devise a plan to solve the problem. \n",
    "Please output the plan starting with the header 'Plan:' and then followed by a numbered list of steps. Do not use past conversation history when you are planning the steps.\n",
    "Please make the plan the minimum number of steps required to accurately complete the task. \n",
    "    \n",
    "These are guidance on when to use a tool to solve a task, follow them strictly: \n",
    "\n",
    "first use \"reverse index search\" tool to get table name \n",
    "if previously step not find anyting，then use \"embedding knn search\" tool to get table name \n",
    "if previously step still can't find the answer, say \"I can't find the answer for this question.\" \n",
    "otherwise use \"db Querying\" tool to search table info using table name which previously step return\n",
    "\n",
    "DO NOT CREATE STEPS THAT ARE NOT NEEDED TO SOLVE A TASK.     \n",
    "Once you have answers for the question, stop and provide the final answers. The final answers should be a combination of the answers to all the questions, not just the last one.\n",
    "\n",
    "Please make sure you have a plan to answer all the questions in the input, not just the last one. \n",
    "Please use these to construct an answer to the question , as though you were answering the question directly. Ensure that your answer is accurate and doesn’t contain any information not directly supported by the summary and quotes. \n",
    "If there are no data or information in this document that seem relevant to this question, please just say \"I can’t find any relevant quotes\".\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"First understand the problem and devise a plan to solve the problem.\"\n",
    "    \" Please output the plan starting with the header 'Plan:' \"\n",
    "    \"and then followed by a numbered list of steps. \"\n",
    "    \"Please make the plan the minimum number of steps required \"\n",
    "    \"to accurately complete the task. If the task is a question, \"\n",
    "    \"the final step should almost always be 'Given the above steps taken, \"\n",
    "    \"please respond to the users original question'. \"\n",
    "    \"\"\"These are guidance to use a tool to solve the task, follow them strictly: \n",
    "first use \"reverse index search\" tool to search  \n",
    "if previously step not find anyting，then use \"embedding knn search\" tool to search   \n",
    "if previously step still can't find the answer, say \"I can't find the answer for this question.\" \n",
    "otherwise use \"db Querying Tool\" tool execute and return \n",
    "\"\"\"\n",
    "    \"At the end of your plan, say '<END_OF_PLAN>'\"\n",
    ")\n",
    "\n",
    "\n",
    "combo_template = \"\"\"\n",
    "First understand the problem and devise a plan to solve the problem. \n",
    "Please output the plan starting with the header 'Plan:' and then followed by a numbered list of steps. Do not use past conversation history when you are planning the steps.\n",
    "Please make the plan the minimum number of steps required to accurately complete the task. \n",
    "    \n",
    "These are guidance on when to use a tool to solve a task, follow them strictly: \n",
    "\n",
    "first use \"reverse index search\" tool to get table name \n",
    "if previously step not find anyting，then use \"embedding knn search\" tool to get table name \n",
    "if previously step still can't find the answer, say \"I can't find the answer for this question.\" \n",
    "otherwise use \"db Querying Tool\" tool to generate sql query using table name which previously step return\n",
    "\n",
    "DO NOT CREATE STEPS THAT ARE NOT NEEDED TO SOLVE A TASK.     \n",
    "Once you have answers for the question, stop and provide the final answers. The final answers should be a combination of the answers to all the questions, not just the last one.\n",
    "\n",
    "Please make sure you have a plan to answer all the questions in the input, not just the last one. \n",
    "Please use these to construct an answer to the question , as though you were answering the question directly. Ensure that your answer is accurate and doesn’t contain any information not directly supported by the summary and quotes. \n",
    "If there are no data or information in this document that seem relevant to this question, please just say \"I can’t find any relevant quotes\". \n",
    "\"\"\"\n",
    "\n",
    "planner = load_chat_planner(sm_llm)\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(combo_template)\n",
    "human_message_prompt = planner.llm_chain.prompt.messages[1]\n",
    "print(system_message_prompt)\n",
    "print(human_message_prompt)\n",
    "planner.llm_chain.prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "executor = load_agent_executor(sm_llm, custom_tool_list, verbose=True)\n",
    "agent = PlanAndExecute(planner=planner, executor=executor, verbose=True, max_iterations=1,memory=memory)\n",
    "\n",
    "\n",
    "#output = agent({\"input\":\"查询‘最近一个月温度合格的派车单数量’这条语句的table name并生成sql\"})\n",
    "output = agent({\"input\":\"最近一个月温度合格的派车单数量\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb9c44-707d-49d9-b226-535758cff3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
