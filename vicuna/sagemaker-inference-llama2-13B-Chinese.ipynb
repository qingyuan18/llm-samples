{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae65fec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy the llama2-13b chinese fine tuned model on Amazon SageMaker\n",
    "\n",
    "As we have finetuned the model, next we will show you how to deploy the model on SageMaker.\n",
    "\n",
    "In this notebook, we explore how to host a large language model on SageMaker using the [Large Model Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-inference.html) container that is optimized for hosting large models using DJLServing. DJLServing is a high-performance universal model serving solution powered by the Deep Java Library (DJL) that is programming language agnostic. To learn more about DJL and DJLServing, you can refer to our recent [blog post](https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2407531",
   "metadata": {},
   "source": [
    "## Create a SageMaker Model for Deployment\n",
    "As a first step, we'll import the relevant libraries and configure several global variables such as the hosting image that will be used nd the S3 location of our model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd75ac31-711d-4fbd-81c8-0d21e76d872b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import serializers, deserializers\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import jinja2\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24862c4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/boto3/compat.py:82: PythonDeprecationWarning: Boto3 will no longer support Python 3.7 starting December 13, 2023. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.8 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "region = sess._region_name # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "\n",
    "s3_client = boto3.client(\"s3\") # client to intreract with S3 API\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\") # client to intreract with SageMaker Endpoints\n",
    "jinja_env = jinja2.Environment() # jinja environment to generate model configuration templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aec093",
   "metadata": {},
   "source": [
    "## Build the inference contianer image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b814406",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile.inference\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile.inference\n",
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "#From 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.23.1-deepspeed0.9.2-cu118\n",
    "From 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\n",
    "#From 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.21.0-deepspeed0.8.3-cu117 \n",
    "\n",
    "ENV LANG=C.UTF-8\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "\n",
    "## Install transfomers version which support LLaMaTokenizer\n",
    "#RUN python3 -m pip install git+https://github.com/huggingface/transformers.git@68d640f7c368bcaaaecfc678f11908ebbd3d6176\n",
    "## Install transfomers version which support vicuna v1.1 LLaMaTokenizer\n",
    "RUN python3 -m pip install transformers==4.30.2\n",
    "#RUN python3 -m pip install git+https://github.com/lanking520/DeepSpeed.git@falcon\n",
    "\n",
    "## Make all local GPUs visible\n",
    "ENV NVIDIA_VISIBLE_DEVICES=\"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "465b7a51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "!aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-west-2.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "351f761d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define repo name, should contain *sagemaker* in the name\n",
    "repo_name = \"sagemaker-llama2-13b-chinese-inference-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a58a7ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script env repo_name=$repo_name bash\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "\n",
    "# The argument to this script is the image name. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "# The name of our algorithm\n",
    "algorithm_name=${repo_name}\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} -f Dockerfile.inference .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84497325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## The image uri which is build and pushed above\n",
    "inference_image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account_id, region, repo_name)\n",
    "inference_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e33506",
   "metadata": {},
   "source": [
    "## Deploying a Large Language Model using Hugging Face Accelerate\n",
    "The DJL Inference Image which we will be utilizing ships with a number of built-in inference handlers for a wide variety of tasks including:\n",
    "- `text-generation`\n",
    "- `question-answering`\n",
    "- `text-classification`\n",
    "- `token-classification`\n",
    "\n",
    "You can refer to this [GitRepo](https://github.com/deepjavalibrary/djl-serving/tree/master/engines/python/setup/djl_python) for a list of additional handlers and available NLP Tasks. <br>\n",
    "These handlers can be utilized as is without having to write any custom inference code. We simply need to create a `serving.properties` text file with our desired hosting options and package it up into a `tar.gz` artifact.\n",
    "\n",
    "Lets take a look at the `serving.properties` file that we'll be using for our first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e3570119",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‚Äòllama2-13b-chinese-accelerate_src‚Äô: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir llama2-13b-chinese-accelerate_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c253cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile llama2-13b-chinese-accelerate_src/serving.properties\n",
    "#engine=Python\n",
    "engine=DeepSpeed\n",
    "option.entryPoint=model.py\n",
    "option.tensor_parallel_degree=4\n",
    "#option.s3url=s3://sagemaker-us-west-2-687912291502/llm/models/LLM_llama2_7b/\n",
    "option.model_id=TheBloke/Llama-2-13B-fp16\n",
    "#option.task=text-generation\n",
    "#option.device_map=auto\n",
    "option.load_in_8bit=TRUE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46cffa8",
   "metadata": {},
   "source": [
    "There are a few options specified here. Lets go through them in turn<br>\n",
    "1. `engine` - specifies the engine that will be used for this workload. In this case we'll be hosting a model using the [DJL Python Engine](https://github.com/deepjavalibrary/djl-serving/tree/master/engines/python)\n",
    "2. `option.entryPoint` - specifies the entrypoint code that will be used to host the model. djl_python.huggingface refers to the `huggingface.py` module from [djl_python repo](https://github.com/deepjavalibrary/djl-serving/tree/master/engines/python/setup/djl_python).  \n",
    "3. `option.s3url` - specifies the location of the model files. Alternativelly an `option.model_id` option can be used instead to specifiy a model from Hugging Face Hub (e.g. `EleutherAI/gpt-j-6B`) and the model will be automatically downloaded from the Hub. The s3url approach is recommended as it allows you to host the model artifact within your own environment and enables faster deployments by utilizing optimized approach within the DJL inference container to transfer the model from S3 into the hosting instance \n",
    "4. `option.task` - This is specific to the `huggingface.py` inference handler and specifies for which task this model will be used\n",
    "5. `option.device_map` - Enables layer-wise model partitioning through [Hugging Face Accelerate](https://huggingface.co/docs/accelerate/usage_guides/big_modeling#designing-a-device-map). With `option.device_map=auto`, Accelerate will determine where to put each **layer** to maximize the use of your fastest devices (GPUs) and offload the rest on the CPU, or even the hard drive if you don‚Äôt have enough GPU RAM (or CPU RAM). Even if the model is split across several devices, it will run as you would normally expect.\n",
    "6. `option.load_in_8bit` - Quantizes the model weights to int8 thereby greatly reducing the memory footprint of the model from the initial FP32. See this [blog post](https://huggingface.co/blog/hf-bitsandbytes-integration) from Hugging Face for additional information \n",
    "\n",
    "For more information on the available options, please refer to the [SageMaker Large Model Inference Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html)\n",
    "\n",
    "Our initial approach here is to utilize the built-in functionality within Hugging Face Transformers to enable Large Language Model hosting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "87923cef-0632-4d8a-b792-4627bf3a0203",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting llama2-13b-chinese-accelerate_src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile llama2-13b-chinese-accelerate_src/requirements.txt\n",
    "protobuf==3.20\n",
    "accelerate>=0.21.0\n",
    "git+https://github.com/huggingface/peft.git\n",
    "xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "97621866-2044-4f6e-b55e-fd90cb8693d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting llama2-13b-chinese-accelerate_src/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile llama2-13b-chinese-accelerate_src/model.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import logging\n",
    "import math\n",
    "import deepspeed\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import transformers\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.models.llama.tokenization_llama import LlamaTokenizer\n",
    "import time\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"transformers version==\"+transformers.__version__)\n",
    "\n",
    "predictor = None\n",
    "\n",
    "\n",
    "\n",
    "def load_model(properties):\n",
    "    tensor_parallel = properties[\"tensor_parallel_degree\"]\n",
    "    model_location = properties['model_dir']\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties['model_id']\n",
    "    #logging.info(f\"Loading model in {model_location}\")\n",
    "    \n",
    "    print(\"----------tensor parallel is {0}---------\".format(tensor_parallel))\n",
    "    \n",
    "    #model_location = \"EleutherAI/gpt-neo-2.7B\"\n",
    "    logging.info(f\"Loading model in {model_location}\")\n",
    "    print(f\"Loading model in {model_location}\")\n",
    "    model_name_or_path = model_location\n",
    "    adapter_name_or_path = 'shareAI/llama2-13b-Chinese-chat'\n",
    "    save_path = '/tmp/llama2-13b-Chinese-chat_v1'\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        #device_map='auto'\n",
    "    )\n",
    "    print(\"load model success\")\n",
    "    model = PeftModel.from_pretrained(model, adapter_name_or_path)\n",
    "    print(\"load adapter success\")\n",
    "    model = model.merge_and_unload()\n",
    "    print(\"merge success\")\n",
    "    \n",
    "    ### for deepspeed ####\n",
    "    model = deepspeed.init_inference(\n",
    "        model,\n",
    "        mp_size=tensor_parallel,\n",
    "        dtype=torch.half,\n",
    "        replace_method=\"auto\",\n",
    "        replace_with_kernel_inject=True,\n",
    "    )\n",
    "\n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "    generator = pipeline(\n",
    "        task=\"text-generation\", model=model, tokenizer=tokenizer, device=local_rank, use_cache=True\n",
    "    )\n",
    "    #model.requires_grad_(False)\n",
    "    #model.eval()\n",
    "    print(\"Ê®°ÂûãÂä†ËΩΩÊàêÂäüÔºÅ\")\n",
    "    ### for accelerate ####\n",
    "    #generator = pipeline(\n",
    "    #    task=\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\", use_cache=True\n",
    "    #)\n",
    "    #generator.tokenizer.pad_token_id = model.config.eos_token_id\n",
    "    return generator, model, tokenizer\n",
    "\n",
    "\n",
    "def handle(inputs: Input) -> None:\n",
    "    global predictor, model, tokenizer\n",
    "    try:\n",
    "        if not predictor:\n",
    "            predictor,model,tokenizer = load_model(inputs.get_properties())\n",
    "\n",
    "        print(inputs)\n",
    "        if inputs.is_empty():\n",
    "            # Model server makes an empty call to warmup the model on startup\n",
    "            return None\n",
    "        \n",
    "        inputs = inputs.get_as_json()\n",
    "        if not inputs.get(\"inputs\"):\n",
    "            return Output().add_as_json({\"code\":-1,\"msg\":\"input field can't be null\"})\n",
    "\n",
    "\n",
    "        #input data\n",
    "        data = inputs.get(\"inputs\")\n",
    "        params = inputs.get(\"parameters\",{})\n",
    "        print(params)\n",
    "        \n",
    "        user_input = '{}</s>'.format(data)\n",
    "        user_input_ids = tokenizer(user_input, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "        model_input_ids = user_input_ids.to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "          outputs = model.generate(\n",
    "                input_ids=model_input_ids, max_new_tokens=params[\"max_new_tokens\"], do_sample=False, top_p=params[\"top_p\"],\n",
    "                temperature=params[\"temperature\"], repetition_penalty=params[\"repetition_penalty\"], eos_token_id=tokenizer.eos_token_id\n",
    "          )\n",
    "        model_input_ids_len = model_input_ids.size(1)\n",
    "        response_ids = outputs[:, model_input_ids_len:]\n",
    "        response = tokenizer.batch_decode(response_ids)\n",
    "        print(\"BotÔºö\" + response[0].strip().replace('</s>', \"\"))\n",
    "    except Exception as e:\n",
    "        return Output().add_as_json({\"code\":-1,\"msg\":e})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d156470a",
   "metadata": {},
   "source": [
    "We place the `serving.properties` file into a tarball and upload it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "12371518",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama2-13b-chinese-accelerate_src/\n",
      "llama2-13b-chinese-accelerate_src/.ipynb_checkpoints/\n",
      "llama2-13b-chinese-accelerate_src/.ipynb_checkpoints/serving-checkpoint.properties\n",
      "llama2-13b-chinese-accelerate_src/model.py\n",
      "llama2-13b-chinese-accelerate_src/serving.properties\n",
      "llama2-13b-chinese-accelerate_src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!tar czvf acc_model.tar.gz llama2-13b-chinese-accelerate_src/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3098668f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-687912291502/llama2-13b-chinese/deploy/code/acc_model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_prefix = \"llama2-13b-chinese/deploy/code\"\n",
    "\n",
    "code_artifact = sess.upload_data(\"acc_model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0807c58",
   "metadata": {},
   "source": [
    "## Deploy Model to a SageMaker Endpoint\n",
    "With a helper function we can now deploy our endpoint and invoke it with some sample inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "20917372",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama2-13b-chinese-chat\n",
      "Image going to be used is ---- > 687912291502.dkr.ecr.us-west-2.amazonaws.com/sagemaker-llama2-13b-chinese-inference-demo:latest\n",
      "Created Model: arn:aws:sagemaker:us-west-2:687912291502:model/llama2-13b-chinese-chat\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "\n",
    "model_name=\"llama2-13b-chinese-chat\"\n",
    "print(model_name)\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": code_artifact\n",
    "    },\n",
    "    \n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f471fd0c-15f4-48cc-840e-4ce155362ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-west-2:687912291502:endpoint-config/llama2-13b-chinese-chat-config',\n",
       " 'ResponseMetadata': {'RequestId': '74ecec12-5c19-4bf4-bf74-125f9fda4c44',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '74ecec12-5c19-4bf4-bf74-125f9fda4c44',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '111',\n",
       "   'date': 'Sat, 19 Aug 2023 08:11:48 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.24xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            #\"VolumeSizeInGB\" : 400,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": 2400,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 15*60,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f48fe9cc-ba30-4135-ab1f-7f5cc50bb79f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Endpoint: arn:aws:sagemaker:us-west-2:687912291502:endpoint/llama2-13b-chinese-chat-endpoint\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ca6e3577-f5dd-4b87-8d9f-a2434c92a952",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-west-2:687912291502:endpoint/llama2-13b-chinese-chat-endpoint\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c19a0c",
   "metadata": {},
   "source": [
    "Let's run an example with a basic text generation prompt Large model inference is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43611bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint llama2-13b-chinese-chat-endpoint of account 687912291502 not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m\n\u001b[1;32m     34\u001b[0m endpoint_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama2-13b-chinese-chat-endpoint\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     35\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     36\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m     37\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0.8\u001b[39m\n\u001b[1;32m     41\u001b[0m }\n\u001b[0;32m---> 43\u001b[0m response_model \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[43mEndpointName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m            \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt_prefix\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mchats_infos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m#\"inputs\": \"‰Ω†Â•Ω\",\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m            \u001b[49m\u001b[43mContentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(response_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#payload = '{\"inputs\" : , \\\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#            \"parameters\": { \"max_length\": 200, \"temperature\": 0.6 }  \\\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#           }'\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#        Body=payload\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m#)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/client.py:980\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    978\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    979\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mValidationError\u001b[0m: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint llama2-13b-chinese-chat-endpoint of account 687912291502 not found."
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import uuid\n",
    "prompt_prefix = \"‰Ω†Ê≠£Âú®‰∏Ä‰∏™ËÅäÂ§©ÂÆ§ÈáåÂíå‰∏çÂêåÂõΩÂÆ∂ÁöÑ‰∫∫‰ª¨ËÅäÂ§©Ôºå‰Ω†ËÉΩËØªÊáÇÊâÄÊúâÂõΩÂÆ∂ÁöÑËØ≠Ë®ÄÔºå‰Ω†Ë¥üË¥£ÈÄöËøáËÅäÂ§©ËÆ∞ÂΩïÂàÜÊûêÊâÄÊúâËÅäÂ§©ËÄÖÁöÑÊÄßÊ†ºÂíåÊúâÊïà‰ø°ÊÅØÔºåÂÖ∑‰ΩìÊ≠•È™§Â¶Ç‰∏ãÔºö\\\n",
    "1.ÈòÖËØª‰ªñ‰ª¨ÁöÑËÅäÂ§©ËÆ∞ÂΩï \\\n",
    "2.ÊÄªÁªì‰ªñ‰ª¨ËÅäÂ§©ÈáåÈù¢ÁöÑÈáçË¶Å‰ø°ÊÅØ \\\n",
    "3.ÊäΩË±°‰ªñ‰ª¨ÁöÑ‰∫∫ËÆæ \\\n",
    "4.‰ΩøÁî®ËØÑÂàÜ‰ΩìÁ≥ªÊäΩË±°‰ªñ‰ª¨‰πãÈó¥ÁöÑ‰∫∫ÈôÖÂÖ≥Á≥ªÔºåÁÑ∂ÂêéÁªô‰∏Ä‰∏™ËØÑÂàÜÔºåËåÉÂõ¥1-10ÂàÜÔºåÂàÜË∂äÈ´òÂÖ≥Á≥ªË∂äÂ•Ω \\\n",
    "ËÅäÂ§©‰ø°ÊÅØÂ¶Ç‰∏ã: \" \n",
    "\n",
    "chats_infos = \"\"\"\n",
    "WaRGazmo : \"you lucked out there buddy\" \n",
    "WarLord : \"suerte? eso no existe \" \n",
    "WarLord : \"soy m√°s r√°pido que la luz \" \n",
    "WaRGazmo : \"it exists.. or karma\" \n",
    "DirtyE1bow : \"so you was a planned birth ?\" \n",
    "WaRGazmo : \"thats what she said bruh\" \n",
    "WarLord : \"te amo mi amor \" \n",
    "Manowarik : \"–ú–∏—Ä –≤–∞–º,–ª—é–¥–∏ –¥–æ–±—Ä—ã–µ..\" \n",
    "kotofei : \"–∏ —Ç–µ–±–µ –±–æ—è—Ä–∏–Ω, —á—Ç–æ –Ω–µ –ø–æ–¥–∞–ª—Å—è –≤ —á–µ–ª—è–¥—å –∫–æ—Ä–æ–ª—é)\" \n",
    "XxNORxXMithra : \"God morgen folkens :) \" \n",
    "kotofei : \"–∏ –ø—Ä–æ—á–∏–µ –∂–∏—Ç–µ–ª–∏ –≥–∞–ª–∞–∫—Ç–∏–∫–∏ \" \n",
    "XxNORxXMithra : \"Ja de ogs√• fors√•vidt :) \" \n",
    "Manowarik : \"–ö–æ—Ç–æ—Ñ–µ–π-—ç—Ç–æ –∫–æ—Ç–æ—Ä—ã–π –ø–æ —Ü–µ–ø–∏ –∫—Ä—É–≥–æ–º?–ü–µ—Å–Ω–∏ —Ç–∞–º,—Å–∫–∞–∑–∫–∏?üòÜüòÜ\" \n",
    "kotofei : \"–Ω–µ, —Ç–æ –¥–∞–ª—å–Ω–∏–π —É–±–æ–≥–∏–π —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫ \" \n",
    "Manowarik : \"–≠—Ö—Ö—Ö..–õ—É–∫–æ–º–æ—Ä—å–µ –º–∏–º–æ..((\" \n",
    "kipl : \"–ö–æ—Ç–æ—Ñ–µ–π –æ–Ω –∏–∑ —Å–∫–∞–∑–∫–∏ –õ–∏—Å–∞ –∏ –ö–æ—Ç–æ—Ñ–µ–π –ò–≤–∞–Ω–æ–≤–∏—á. \" \n",
    "kipl : \"–ú–µ–∂–≤–∏–¥–æ–≤–æ–π –±—Ä–∞–∫ –∏ –∫—Ä—ã—à–µ–≤–∞–Ω–∏–µ –ª–µ—Å–∞\" \n",
    "kotofei : \"–ª–∏—Å–∞ ü¶ä –º–µ—Ä–∞–ª–∏—Å–∞ –∏ –ö–æ—Ç–æ—Ñ–µ–π –ò–≤–∞–Ω—ã—á \" \n",
    "leister : \"üòÜ\" \n",
    "XxFoxyQBAxX : \"po co tyle zrobi≈Çe≈õ?\"\n",
    "\"\"\"\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "endpoint_name = 'llama2-13b-chinese-chat-endpoint'\n",
    "parameters = {\n",
    "  \"max_length\": 2048,\n",
    "  \"temperature\": 1,\n",
    "  \"max_new_tokens\":500,\n",
    "  \"repetition_penalty\": 0.8,\n",
    "  \"top_p\":0.8\n",
    "}\n",
    "\n",
    "response_model = client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=json.dumps(\n",
    "            {\n",
    "                \"inputs\": [prompt_prefix+chats_infos],\n",
    "                #\"inputs\": \"‰Ω†Â•Ω\",\n",
    "                \"parameters\": parameters,\n",
    "            }\n",
    "            ),\n",
    "            ContentType=\"application/json\",\n",
    "        )\n",
    "\n",
    "print(response_model['Body'].read().decode('utf8'))\n",
    "\n",
    "#payload = '{\"inputs\" : , \\\n",
    "#            \"parameters\": { \"max_length\": 200, \"temperature\": 0.6 }  \\\n",
    "#           }'\n",
    "#encoded_inp = (payload).encode(\"utf-8\")\n",
    "#response = client.invoke_endpoint(\n",
    "#        EndpointName=endpoint_name,\n",
    "#        ContentType='application/json',\n",
    "#        Accept='application/json',\n",
    "#        Body=payload\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2da0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the endpoint before proceeding\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7377c84c",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a842b9",
   "metadata": {},
   "source": [
    "[sagemaker-hosting/Large-Language-Model-Hosting/](https://github.com/aws-samples/sagemaker-hosting/tree/main/Large-Language-Model-Hosting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
